{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Build\n",
    "\n",
    "---\n",
    "**Deep learning -----> training a neural network.**\n",
    "\n",
    "\n",
    "### Components of Neural Network\n",
    "---\n",
    "\n",
    "- An input layer, x\n",
    "- An arbitary amount of hidden layers\n",
    "- An output layer , ŷ\n",
    "- A set of weight and biases between each layer, W and b  \n",
    "- The choice of activation function betweens each hidden layer, ex:- σ Sigmoid \n",
    "\n",
    "\n",
    "![2 layer NN](https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png)\n",
    "**2 Layer Neural Network**\n",
    "\n",
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,x,y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network\n",
    "\n",
    "The output ŷ of a simple 2-layer Neural Network is:\n",
    "![](https://miro.medium.com/max/710/1*E1_l8PGamc2xTNS87XGNcA.png)\n",
    "\n",
    "The weights W and biases b are the only variables that affects the output ŷ.\n",
    "\n",
    "Naturally the right value of weights and biases determines the strength of predictions. The process of fine-tuning the weights and biases from input data is known as **Training the Nueral Network**. \n",
    "\n",
    "Each iteration of this training process consists of the following steps :\n",
    "\n",
    "- Calculating the predicted output ŷ, known as **feedforward**\n",
    "- Updating weights and biases, known as **backpropagation**\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*CEtt0h8Rss_qPu7CyqMTdQ.png)\n",
    "\n",
    "### Feed Forward\n",
    "\n",
    "FeedForward is simple calculus and for a basic 2-layer neural network, the output of the Neural Network is :\n",
    "\n",
    "![](https://miro.medium.com/max/710/1*E1_l8PGamc2xTNS87XGNcA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,x,y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "There are many loss functions available, depending on the nature of the problem the loss function should be selected. \n",
    "loss function taken : **sum-of-squares error**\n",
    "\n",
    "![](https://miro.medium.com/max/600/1*iNa1VLdaeqwUAxpNXs3jwQ.png)\n",
    "\n",
    "- The sum-of-squares error is sum of diffrences between each predicted value and actual value, it is then squared to measure the absolute value. \n",
    "- **The goal in training is to find the best set of wieghts and biases that minimizes the loss functions.**\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "- We have measured the error of our prediction (loss), we need a way to propogate the error back and to update our weights and biases.\n",
    "- Inorder to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the wieght and biases.\n",
    "\n",
    "Derivative of a function is slope of the function : calculus \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*3FgDOt4kJxK2QZlb9T0cpg.png)\n",
    "\n",
    "- If we have derivative, we can update the biases with increasing/reducing with it. This is known as **gradient descent**\n",
    "- We can directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function does not contain them. Therefore, we need **chain rule** to calculate it. \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*7zxb2lfWWKaVxnmq2o69Mw.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "    \n",
    "    def backprop(self):\n",
    "        # Application of chain rule to find the derivative of loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "        \n",
    "        #Update weights with derivative (slope) of the loss functions\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of calculus : https://www.youtube.com/watch?v=tIeHLnjs5U8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
